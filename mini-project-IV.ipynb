{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loan predictions\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We want to automate the loan eligibility process based on customer details that are provided as online application forms are being filled. You can find the dataset [here](https://drive.google.com/file/d/1h_jl9xqqqHflI5PsuiQd_soNYxzFfjKw/view?usp=sharing). These details concern the customer's Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and other things as well. \n",
    "\n",
    "Variable - Description<br>\n",
    "Loan_ID - Unique Loan ID<br>\n",
    "Gender - Male/ Female<br>\n",
    "Married -  Applicant married (Y/N)<br>\n",
    "Dependents - Number of dependents<br>\n",
    "Education - Applicant Education (Graduate/ Under Graduate)<br>\n",
    "Self_Employed - Self employed (Y/N)<br>\n",
    "ApplicantIncome - Applicant income<br>\n",
    "CoapplicantIncome - Coapplicant income<br>\n",
    "LoanAmount - Loan amount in thousands<br>\n",
    "Loan_Amount_Term - Term of loan in months<br>\n",
    "Credit_History - credit history meets guidelines<br>\n",
    "Property_Area - Urban/ Semi Urban/ Rural<br>\n",
    "Loan_Status - Loan approved (Y/N)<br>\n",
    "\n",
    "\n",
    "\n",
    "### Explore the problem in following stages:\n",
    "\n",
    "1. Hypothesis Generation – understanding the problem better by brainstorming possible factors that can impact the outcome\n",
    "2. Data Exploration – looking at categorical and continuous feature summaries and making inferences about the data.\n",
    "3. Data Cleaning – imputing missing values in the data and checking for outliers\n",
    "4. Feature Engineering – modifying existing variables and creating new ones for analysis\n",
    "5. Model Building – making predictive models on the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hypothesis Generation\n",
    "\n",
    "Generating a hypothesis is a major step in the process of analyzing data. This involves understanding the problem and formulating a meaningful hypothesis about what could potentially have a good impact on the outcome. This is done BEFORE looking at the data, and we end up creating a laundry list of the different analyses which we can potentially perform if data is available.\n",
    "\n",
    "#### Possible hypotheses\n",
    "Which applicants are more likely to get a loan\n",
    "\n",
    "1. Applicants having a credit history \n",
    "2. Applicants with higher applicant and co-applicant incomes\n",
    "3. Applicants with higher education level\n",
    "4. Properties in urban areas with high growth perspectives\n",
    "\n",
    "Do more brainstorming and create some hypotheses of your own. Remember that the data might not be sufficient to test all of these, but forming these enables a better understanding of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "Let's do some basic data exploration here and come up with some inferences about the data. Go ahead and try to figure out some irregularities and address them in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key challenges in any data set are missing values. Lets start by checking which columns contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Credit_History'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.apply(lambda x: sum(x.isnull()),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some basic statistics for numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many applicants have a `Credit_History`? (`Credit_History` has value 1 for those who have a credit history and 0 otherwise)\n",
    "2. Is the `ApplicantIncome` distribution in line with your expectation? Similarly, what about `CoapplicantIncome`?\n",
    "3. Tip: Can you see a possible skewness in the data by comparing the mean to the median, i.e. the 50% figure of a feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's discuss nominal (categorical) variable. Look at the number of unique values in each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#turn loan status into binary \n",
    "modified=df\n",
    "modified['Loan_Status']=df['Loan_Status'].apply(lambda x: 0 if x==\"N\" else 1 )\n",
    "#calculate the mean\n",
    "modified.groupby('Credit_History').mean()['Loan_Status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore further using the frequency of different categories in each nominal variable. Exclude the ID obvious reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution analysis\n",
    "\n",
    "Study distribution of various variables. Plot the histogram of ApplicantIncome, try different number of bins.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df.ApplicantIncome,kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Look at box plots to understand the distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df.ApplicantIncome.dropna(),kde=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the distribution of income segregated  by `Education`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x='Education',y='ApplicantIncome',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the histogram and boxplot of LoanAmount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(x='LoanAmount', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='LoanAmount',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(x='Loan_Amount_Term', data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be some extreme values. Both `ApplicantIncome` and `LoanAmount` require some amount of data munging. `LoanAmount` has missing and well as extreme values values, while `ApplicantIncome` has a few extreme values, which demand deeper understanding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical variable analysis\n",
    "\n",
    "Try to understand categorical variables in more details using `pandas.DataFrame.pivot_table` and some visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame.pivot_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning\n",
    "\n",
    "This step typically involves imputing missing values and treating outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Values\n",
    "\n",
    "Missing values may not always be NaNs. For instance, the `Loan_Amount_Term` might be 0, which does not make sense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute missing values for all columns. Use the values which you find most meaningful (mean, mode, median, zero.... maybe different mean values for different groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values\n",
    "#categorical\n",
    "df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)\n",
    "df['Married'].fillna(df['Married'].mode()[0], inplace=True)\n",
    "df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)\n",
    "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "df['Credit_History'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)\n",
    "df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)\n",
    "df['Self_Employed'].fillna(df['Self_Employed'].mode()[0], inplace=True)\n",
    "\n",
    "#numerical\n",
    "df['LoanAmount'].fillna(df['LoanAmount'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme values\n",
    "Try a log transformation to get rid of the extreme values in `LoanAmount`. Plot the histogram before and after the transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine both incomes as total income and take a log transformation of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create TotalIncome column as a sum of ApplicantIncome and CoapplicantIncome\n",
    "df['TotalIncome']=df['ApplicantIncome']+df['CoapplicantIncome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#create TotalIncome_log column as a log of TotalIncome\n",
    "df['TotalIncome_log']=np.log(df['TotalIncome'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#create LoanAmount_log column\n",
    "df['LoanAmount_log']=np.log(df['LoanAmount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['ApplicantIncome','CoapplicantIncome','LoanAmount', 'TotalIncome'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA CLEANING WITH PIPELINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class LogDfTransform(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columnNames):\n",
    "        self.columnNames = columnNames\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X=X.copy()\n",
    "        X.loc[:,self.columnNames]=np.log(X[self.columnNames]).values\n",
    "        return X\n",
    "    \n",
    "income_log = LogDfTransform(['ApplicantIncome', 'LoanAmount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for our pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# create pipelines for numerical and categorical columns\n",
    "# pipeline for numerical columns (log transform -> imputation -> standard scaler -> selectkbest)\n",
    "numerical_transform = Pipeline([('impute_mean', SimpleImputer(strategy='mean')),\n",
    "                                ('scaling', StandardScaler()),\n",
    "                                ('select_kbest', SelectKBest(k=3))])\n",
    "\n",
    "# pipeline for categorical columns\n",
    "categorical_transform = Pipeline([('impute_mode', SimpleImputer(strategy='most_frequent')),\n",
    "                                ('one-hot-encode', OneHotEncoder())])\n",
    "\n",
    "# columntransformer for numerical and categorical columns\n",
    "preprocessing_df = ColumnTransformer([('numerical', numerical_transform, ['TotalIncome_log', 'LoanAmount_log','Loan_Amount_Term','Credit_History']),\n",
    "('categorical', categorical_transform, ['Gender', 'Married', 'Dependents', 'Education', \n",
    "'Self_Employed', 'Property_Area'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create a LogisticRegression classifier\n",
    "logistic = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# build a pipeline for our model\n",
    "pipeline = Pipeline([('preprocessing', preprocessing_df),\n",
    "                    ('classifier', logistic)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split data into training and test sets\n",
    "X=df.drop(['Loan_Status', 'Loan_ID'], axis=1)\n",
    "y=df['Loan_Status']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try paramater grid search to improve the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# find the best parameters for the model using GridSearchCV\n",
    "param_grid = {'features__random_forest__max_depth': [5, 10, 15, 20, 25, 30],\n",
    "                'features__random_forest__n_estimators': [100, 200, 300, 400, 500],\n",
    "                'features__decision_tree__max_depth': [5, 10, 15, 20, 25, 30],\n",
    "                'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# create gridsearch object\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# fit grid search\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "best_model = grid.best_estimator_\n",
    "best_hyperparams = grid.best_params_\n",
    "best_acc = grid.score(X_test, y_test)\n",
    "print(f'Best test set accuracy: {best_acc}\\nAchieved with hyperparameters: {best_hyperparams}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 5. Using Pipeline\n",
    "If you didn't use pipelines before, transform your data prep, feat. engineering and modeling steps into Pipeline. It will be helpful for deployment.\n",
    "\n",
    "The goal here is to create the pipeline that will take one row of our dataset and predict the probability of being granted a loan.\n",
    "\n",
    "`pipeline.predict(x)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# get accuracy score\n",
    "print('Accuracy: ', accuracy_score(y_test, pipeline.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display HTML representation in a jupyter context\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, save the HTML to a file\n",
    "from sklearn.utils import estimator_html_repr\n",
    "\n",
    "with open('model_pipeline.html', 'w') as f:  \n",
    "    f.write(estimator_html_repr(pipeline))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tester = X_test.iloc[[5]]\n",
    "y_pred = pipeline.predict(X_tester)\n",
    "print(type(X_tester))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tester.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deploy your model to cloud and test it with PostMan, BASH or Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#store in pickle\n",
    "pickle.dump(pipeline, open('model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testmodel = pickle.load(open('model.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prpe = testmodel.predict(X_tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_prpe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'ec2-52-14-229-23.us-east-2.compute.amazonaws.com:5000'\n",
    "json_entry = {\n",
    "    \"Gender\": \"Male\",\n",
    "    \"Married\": \"No\",\n",
    "    \"Dependents\": 1,\n",
    "    \"Education\": \"Graduate\",\n",
    "    \"Self_Employed\": \"No\",\n",
    "    \"ApplicantIncome\": 2345,\n",
    "    \"CoapplicantIncome\": 0,\n",
    "    \"LoanAmount\": 128.0,\n",
    "    \"Loan_Amount_Term\": 360.0,\n",
    "    \"Credit_History\": 1.0,\n",
    "    \"Property_Area\": \"Urban\"\n",
    "}\n",
    "\n",
    "import requests\n",
    "res = requests.post(url, json=json_entry)\n",
    "if res.ok:\n",
    "    print(res.json())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4a3c54e841fe8ed729f88f1a4dbf5907a70c4a9986278aeeef34dd666b803d5d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
